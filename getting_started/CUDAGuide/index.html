<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>CUDA Guide - Enzyme AD</title><meta name=description content="Enzyme Automatic Differentiation Framework"><meta name=generator content="Hugo 0.74.3"><link href=https://enzyme.mit.edu/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://enzyme.mit.edu/getting_started/CUDAGuide/><link rel=stylesheet href=https://enzyme.mit.edu/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://enzyme.mit.edu/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://enzyme.mit.edu/js/bundle.js></script><link rel=apple-touch-icon sizes=180x180 href=/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon_io/favicon-16x16.png><link rel=manifest href=/favicon_io/site.webmanifest><script defer data-domain=enzyme.mit.edu src=https://plausible.io/js/plausible.js></script><style>:root{}</style></head><body><div class=container><header><h1><div><a href=/ style=text-decoration:none;color:#000><img src=https://enzyme.mit.edu//logo.svg width=40px align=absmiddle>
Enzyme AD</a></div></h1><p class=description>Enzyme Automatic Differentiation Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://groups.google.com/d/forum/enzyme-dev>Discussion List</a></li></ul></li><li><a href=https://github.com/wsmoses/Enzyme>Source</a></li><li><a href=https://github.com/wsmoses/Enzyme/issues>Bugs</a></li><li><a href=/explorer>Try Online</a></li><li><a href=/getting_started/Faq/>FAQ</a></li></ul></nav></div><div class=content-container><main><h1>CUDA Guide</h1><h2 id=reference-c-example>Reference C++ example&nbsp;<a class=headline-hash href=#reference-c-example>¶</a></h2><blockquote><p><strong>WARNING</strong>: CUDA support is highly experimental and in active development.</p></blockquote><p>Suppose we wanted to port the following C++ code to CUDA, with Enzyme autodiff support:</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=cp>#include</span> <span class=cpf>&lt;stdio.h&gt;</span><span class=cp>
</span><span class=cp></span>
<span class=kt>void</span> <span class=nf>foo</span><span class=p>(</span><span class=kt>double</span><span class=o>*</span> <span class=n>x_in</span><span class=p>,</span> <span class=kt>double</span> <span class=o>*</span><span class=n>x_out</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>x_out</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>x_in</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=n>x_in</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>
<span class=p>}</span>


<span class=kt>int</span> <span class=n>enzyme_dup</span><span class=p>;</span>
<span class=kt>int</span> <span class=n>enzyme_out</span><span class=p>;</span>
<span class=kt>int</span> <span class=n>enzyme_const</span><span class=p>;</span>

<span class=k>typedef</span> <span class=nf>void</span> <span class=p>(</span><span class=o>*</span><span class=n>f_ptr</span><span class=p>)(</span><span class=kt>double</span><span class=o>*</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span><span class=p>);</span>

<span class=k>extern</span> <span class=kt>void</span> <span class=nf>__enzyme_autodiff</span><span class=p>(</span><span class=n>f_ptr</span><span class=p>,</span>
    <span class=kt>int</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span><span class=p>,</span>
    <span class=kt>int</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span><span class=p>);</span>

<span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>

    <span class=kt>double</span> <span class=n>x</span> <span class=o>=</span> <span class=mf>1.4</span><span class=p>;</span>
    <span class=kt>double</span> <span class=n>d_x</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>;</span>
    <span class=kt>double</span> <span class=n>y</span><span class=p>;</span>
    <span class=kt>double</span> <span class=n>d_y</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>;</span>

    <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>foo</span><span class=p>,</span>
        <span class=n>enzyme_dup</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>x</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>d_x</span><span class=p>,</span>
        <span class=n>enzyme_dup</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>y</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>d_y</span><span class=p>);</span>

    <span class=n>printf</span><span class=p>(</span><span class=s>&#34;%f %f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>);</span>
    <span class=n>printf</span><span class=p>(</span><span class=s>&#34;%f %f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>d_x</span><span class=p>,</span> <span class=n>d_y</span><span class=p>);</span>

<span class=p>}</span>
</code></pre></div><p>A one-liner compilation of the above using Enzyme:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>clang test2.cpp -Xclang -load -Xclang /path/to/ClangEnzyme-11.so -O2 -fno-vectorize -fno-unroll-loops
</code></pre></div><h2 id=cuda-example>CUDA Example&nbsp;<a class=headline-hash href=#cuda-example>¶</a></h2><p>When porting the above code, there are some caveats to be aware of:</p><ol><li>CUDA 10.1 is the latest supported CUDA at the time of writing (Jan/20/2021) for LLVM 11.</li><li><code>__enzyme_autodiff</code> should only be invoked on <code>___device___</code> code, not <code>__global__</code> kernel code. <code>__global__</code> kernels may be supported in the future.</li><li><code>--cuda-gpu-arch=sm_xx</code> is usually needed as the default <code>sm_20</code> is unsupported by modern CUDA versions.</li></ol><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=cp>#include</span> <span class=cpf>&lt;stdio.h&gt;</span><span class=cp>
</span><span class=cp></span>
<span class=kt>void</span> <span class=n>__device__</span> <span class=nf>foo_impl</span><span class=p>(</span><span class=kt>double</span><span class=o>*</span> <span class=n>x_in</span><span class=p>,</span> <span class=kt>double</span> <span class=o>*</span><span class=n>x_out</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>x_out</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>x_in</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=n>x_in</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>    
<span class=p>}</span>

<span class=k>typedef</span> <span class=nf>void</span> <span class=p>(</span><span class=o>*</span><span class=n>f_ptr</span><span class=p>)(</span><span class=kt>double</span><span class=o>*</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span><span class=p>);</span>

<span class=k>extern</span> <span class=kt>void</span> <span class=n>__device__</span> <span class=nf>__enzyme_autodiff</span><span class=p>(</span><span class=n>f_ptr</span><span class=p>,</span>
    <span class=kt>int</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span><span class=p>,</span>
    <span class=kt>int</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span>
<span class=p>);</span>

<span class=kt>void</span> <span class=n>__global__</span> <span class=nf>foo</span><span class=p>(</span><span class=kt>double</span><span class=o>*</span> <span class=n>x_in</span><span class=p>,</span> <span class=kt>double</span> <span class=o>*</span><span class=n>x_out</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>foo_impl</span><span class=p>(</span><span class=n>x_in</span><span class=p>,</span> <span class=n>x_out</span><span class=p>);</span>
<span class=p>}</span>

<span class=kt>int</span> <span class=n>__device__</span> <span class=n>enzyme_dup</span><span class=p>;</span>
<span class=kt>int</span> <span class=n>__device__</span> <span class=n>enzyme_out</span><span class=p>;</span>
<span class=kt>int</span> <span class=n>__device__</span> <span class=n>enzyme_const</span><span class=p>;</span>

<span class=kt>void</span> <span class=n>__global__</span> <span class=nf>foo_grad</span><span class=p>(</span><span class=kt>double</span><span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=kt>double</span> <span class=o>*</span><span class=n>d_x</span><span class=p>,</span> <span class=kt>double</span> <span class=o>*</span><span class=n>y</span><span class=p>,</span> <span class=kt>double</span> <span class=o>*</span><span class=n>d_y</span><span class=p>)</span> <span class=p>{</span>

    <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>foo_impl</span><span class=p>,</span>
        <span class=n>enzyme_dup</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>d_x</span><span class=p>,</span>
        <span class=n>enzyme_dup</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>d_y</span><span class=p>);</span>

<span class=p>}</span>

<span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>

    <span class=kt>double</span> <span class=o>*</span><span class=n>x</span><span class=p>,</span> <span class=o>*</span><span class=n>d_x</span><span class=p>,</span> <span class=o>*</span><span class=n>y</span><span class=p>,</span> <span class=o>*</span><span class=n>d_y</span><span class=p>;</span> <span class=c1>// device pointers
</span><span class=c1></span>
    <span class=n>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>x</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>x</span><span class=p>));</span>
    <span class=n>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>d_x</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>d_x</span><span class=p>));</span>
    <span class=n>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>y</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>y</span><span class=p>));</span>
    <span class=n>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>d_y</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>d_y</span><span class=p>));</span>

    <span class=kt>double</span> <span class=n>host_x</span> <span class=o>=</span> <span class=mf>1.4</span><span class=p>;</span>
    <span class=kt>double</span> <span class=n>host_d_x</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>;</span>
    <span class=kt>double</span> <span class=n>host_y</span><span class=p>;</span>
    <span class=kt>double</span> <span class=n>host_d_y</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>;</span>

    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>host_x</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>x</span><span class=p>),</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>
    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>d_x</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>host_d_x</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>d_x</span><span class=p>),</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>
    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>host_y</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>y</span><span class=p>),</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>
    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>d_y</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>host_d_y</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>d_y</span><span class=p>),</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>

    <span class=c1>// foo&lt;&lt;&lt;1,1&gt;&gt;&gt;(x, y); fwd-pass only
</span><span class=c1></span>    <span class=n>foo_grad</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>d_x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>d_y</span><span class=p>);</span> <span class=c1>// fwd and bkwd pass
</span><span class=c1></span>
    <span class=n>cudaDeviceSynchronize</span><span class=p>();</span> <span class=c1>// synchroniz
</span><span class=c1></span>
    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=o>&amp;</span><span class=n>host_x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>x</span><span class=p>),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=o>&amp;</span><span class=n>host_d_x</span><span class=p>,</span> <span class=n>d_x</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>d_x</span><span class=p>),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=o>&amp;</span><span class=n>host_y</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>y</span><span class=p>),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
    <span class=n>cudaMemcpy</span><span class=p>(</span><span class=o>&amp;</span><span class=n>host_d_y</span><span class=p>,</span> <span class=n>d_y</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=o>*</span><span class=n>d_y</span><span class=p>),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>

    <span class=n>printf</span><span class=p>(</span><span class=s>&#34;%f %f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>host_x</span><span class=p>,</span> <span class=n>host_y</span><span class=p>);</span>
    <span class=n>printf</span><span class=p>(</span><span class=s>&#34;%f %f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>host_d_x</span><span class=p>,</span> <span class=n>host_d_y</span><span class=p>);</span>

<span class=p>}</span>
</code></pre></div><p>For convenience, a one-liner compilation step is (against sm_70):</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>clang test3.cu -Xclang -load -Xclang /path/to/ClangEnzyme-11.so -O2 -fno-vectorize -fno-unroll-loops -fPIC --cuda-gpu-arch<span class=o>=</span>sm_70 -lcudart -L/usr/local/cuda-10.1/lib64
</code></pre></div><p>Note that this procedure (using ClangEnzyme as opposed to LLVMEnzyme manually) may not properly nest Enzyme between optimization passes and may impact performance in unintended ways.</p><h2 id=heterogeneous-ad>Heterogeneous AD&nbsp;<a class=headline-hash href=#heterogeneous-ad>¶</a></h2><p>It is often desirable to take derivatives of programs that run in part on the CPU and in part on the GPU. By placing a call to <code>__enzyme_autodiff</code> in a GPU kernel like above, one can successfully take the derivative of GPU programs. Similarly one can use <code>__enzyme_autodiff</code> within CPU programs to differentiate programs which run entirely on the CPU. Unfortunately, differentiating functions that call GPU kernels requires a bit of extra work (shown below) &ndash; largely to work around the lack of support within LLVM for modules with multiple architecture targets.</p><p>To successfully differentiate across devices, we will use Enzyme on the GPU to export the augmented forward pass and reverse pass of the kernel being called, and then use Enzyme&rsquo;s custom derivative support to import that derivative function into the CPU code. This then allows Enzyme to differentiate any CPU code that also calls the kernel.</p><p>Suppose we have a heterogeneous program like the following:</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp>
<span class=c1>// GPU Kernel
</span><span class=c1></span><span class=n>__global__</span> 
<span class=kt>void</span> <span class=nf>collide</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>size_t</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
    <span class=k>if</span> <span class=p>(</span><span class=n>idx</span> <span class=o>&lt;</span> <span class=mi>100</span><span class=p>)</span> <span class=p>{</span>
        <span class=n>dsr</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>+=</span> <span class=n>src</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>*</span> <span class=n>src</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>-</span> <span class=mi>3</span> <span class=o>*</span> <span class=n>src</span><span class=p>[</span><span class=n>idx</span><span class=p>];</span>
    <span class=p>}</span>
<span class=p>}</span>

<span class=c1>// Wrapper CPU function which calls kernel
</span><span class=c1></span><span class=kt>void</span> <span class=nf>kern</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>collide</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>dst</span><span class=p>);</span>
<span class=p>}</span>

<span class=c1>// Main CPU code that calls wrapper function
</span><span class=c1></span><span class=kt>void</span> <span class=nf>iter</span><span class=p>(</span><span class=kt>int</span> <span class=n>nTimeSteps</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>)</span> <span class=p>{</span>
    <span class=k>for</span> <span class=p>(</span><span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>nTimeSteps</span><span class=o>/</span><span class=mi>2</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
        <span class=n>kern</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>dst</span><span class=p>);</span>
        <span class=n>kern</span><span class=p>(</span><span class=n>dst</span><span class=p>,</span> <span class=n>src</span><span class=p>);</span>
    <span class=p>}</span>
<span class=p>}</span>

</code></pre></div><p>We would first try to differentiate the CPU side by calling <code>__enzyme_autodiff</code> on <code>iter</code> as shown below:</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=k>template</span> <span class=o>&lt;</span><span class=k>typename</span><span class=p>...</span> <span class=n>Args</span><span class=o>&gt;</span>
<span class=kt>void</span> <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>Args</span><span class=p>...);</span>

<span class=kt>void</span> <span class=nf>grad_iter</span><span class=p>(</span><span class=kt>int</span> <span class=n>nTimeSteps</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>)</span> <span class=p>{</span>
  <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>iter</span><span class=p>,</span> <span class=n>nTimeSteps</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>);</span>
<span class=p>}</span>
</code></pre></div><p>Enzyme, however, would return an error saying it cannot differentiate through a CUDA call, which appears like the following:</p><pre><code>declare dso_local i32 @__cudaPushCallConfiguration(i64, i32, i64, i32, i64, i8*) local_unnamed_addr #2

clang-13: /home/wmoses/git/Enzyme/enzyme/Enzyme/EnzymeLogic.cpp:1459: const AugmentedReturn&amp; EnzymeLogic::CreateAugmentedPrimal(llvm::Function*, DIFFE_TYPE, const std::vector&lt;DIFFE_TYPE&gt;&amp;, llvm::TargetLibraryInfo&amp;, TypeAnalysis&amp;, bool, const FnTypeInfo&amp;, std::map&lt;llvm::Argument*, bool&gt;, bool, bool, bool, bool): Assertion `0 &amp;&amp; &quot;attempting to differentiate function without definition&quot;' failed.
PLEASE submit a bug report to https://bugs.llvm.org/ and include the crash backtrace, preprocessed source, and associated run script.
</code></pre><p>To remedy this, we can use Enzyme&rsquo;s custom derivative registration to define a custom forward and reverse pass for the wrapper function <code>kern</code> as follows:</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=c1>// We move the body of collide into a separate device function collide_body to allow us
</span><span class=c1>// to pass collide_body to various differentiation methods. This is necessary as differentiation
</span><span class=c1>// can only be done on device, not global kernel functions.
</span><span class=c1></span><span class=n>__device__</span>
<span class=kt>void</span> <span class=nf>collide_body</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>size_t</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
    <span class=k>if</span> <span class=p>(</span><span class=n>idx</span> <span class=o>&lt;</span> <span class=mi>100</span><span class=p>)</span> <span class=p>{</span>
        <span class=n>dst</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>+=</span> <span class=n>src</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>*</span> <span class=n>src</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>-</span> <span class=mi>3</span> <span class=o>*</span> <span class=n>src</span><span class=p>[</span><span class=n>idx</span><span class=p>];</span>
    <span class=p>}</span>
<span class=p>}</span>

<span class=c1>// GPU Kernel
</span><span class=c1></span><span class=n>__global__</span>
<span class=kt>void</span> <span class=nf>collide</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>collide_body</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>dst</span><span class=p>);</span>
<span class=p>}</span>

<span class=c1>// Wrapper CPU function which calls kernel
</span><span class=c1></span><span class=n>__attribute__</span><span class=p>((</span><span class=n>noinline</span><span class=p>))</span>
<span class=kt>void</span> <span class=n>kern</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>collide</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>dst</span><span class=p>);</span>
<span class=p>}</span>

<span class=c1>// Main CPU code that calls wrapper function
</span><span class=c1></span><span class=kt>void</span> <span class=nf>iter</span><span class=p>(</span><span class=kt>int</span> <span class=n>nTimeSteps</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>)</span> <span class=p>{</span>
    <span class=k>for</span> <span class=p>(</span><span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>nTimeSteps</span><span class=o>/</span><span class=mi>2</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
        <span class=n>kern</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>dst</span><span class=p>);</span>
        <span class=n>kern</span><span class=p>(</span><span class=n>dst</span><span class=p>,</span> <span class=n>src</span><span class=p>);</span>
    <span class=p>}</span>
<span class=p>}</span>

<span class=k>template</span> <span class=o>&lt;</span><span class=k>typename</span><span class=p>...</span> <span class=n>Args</span><span class=o>&gt;</span>
<span class=kt>void</span> <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>Args</span><span class=p>...);</span>

<span class=kt>void</span> <span class=nf>grad_iter</span><span class=p>(</span><span class=kt>int</span> <span class=n>nTimeSteps</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>iter</span><span class=p>,</span> <span class=n>nTimeSteps</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>);</span>
<span class=p>}</span>

<span class=c1>// A function similar to __enzyme_autodiff, except it only calls the augmented forward pass, returning
</span><span class=c1>// a tape structure to hold any values that may be overwritten and needed for the reverse.
</span><span class=c1></span><span class=k>template</span> <span class=o>&lt;</span><span class=k>typename</span><span class=p>...</span> <span class=n>Args</span><span class=o>&gt;</span>
<span class=n>__device__</span> <span class=kt>void</span><span class=o>*</span> <span class=n>__enzyme_augmentfwd</span><span class=p>(</span><span class=n>Args</span><span class=p>...);</span>

<span class=c1>// A function similar to __enzyme_autodiff, except it only calls the revese pass, taking in the tape
</span><span class=c1>// as its last argument.
</span><span class=c1></span><span class=k>template</span> <span class=o>&lt;</span><span class=k>typename</span><span class=p>...</span> <span class=n>Args</span><span class=o>&gt;</span>
<span class=n>__device__</span> <span class=kt>void</span> <span class=n>__enzyme_reverse</span><span class=p>(</span><span class=n>Args</span><span class=p>...);</span>

<span class=c1>// A wrapper GPU kernel for calling the forward pass of collide. The wrapper code stores
</span><span class=c1>// the tape generated by Enzyme into a unique location per thread
</span><span class=c1></span><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>aug_collide</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>,</span> <span class=kt>void</span><span class=o>**</span> <span class=n>tape</span><span class=p>)</span>
<span class=p>{</span>
    <span class=n>size_t</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
    <span class=n>tape</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>__enzyme_augmentfwd</span><span class=p>((</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>collide_body</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>);</span>
<span class=p>}</span>

<span class=c1>// A wrapper GPU kernel for calling the reverse pass of collide. The wrapper code retrieves
</span><span class=c1>// the corresponding tape per thread being executed.
</span><span class=c1></span><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>rev_collide</span><span class=p>(</span> <span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>,</span> <span class=kt>void</span><span class=o>**</span> <span class=n>tape</span><span class=p>)</span>
<span class=p>{</span>
    <span class=n>size_t</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
    <span class=n>__enzyme_reverse</span><span class=p>((</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>collide_body</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>,</span> <span class=n>tape</span><span class=p>[</span><span class=n>idx</span><span class=p>]);</span>
<span class=p>}</span>

<span class=c1>// The augmented forward pass of the CPU kern call, allocating and returning
</span><span class=c1>// tape memory  needed to compute the reverse pass. This calls a augmented collide
</span><span class=c1>// GPU kernel, passing in a unique 8-byte location to store the tape.
</span><span class=c1></span><span class=kt>void</span><span class=o>*</span> <span class=nf>aug_kern</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>)</span> <span class=p>{</span>
    <span class=kt>void</span><span class=o>**</span> <span class=n>tape</span><span class=p>;</span>
    <span class=n>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>tape</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span> <span class=o>*</span> <span class=cm>/*total number of threads*/</span><span class=mi>100</span><span class=p>);</span>
    <span class=n>aug_collide</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>,</span> <span class=n>tape</span><span class=p>);</span>
    <span class=k>return</span> <span class=p>(</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>tape</span><span class=p>;</span>
<span class=p>}</span>

<span class=c1>// The reverse pass of the CPU kern call, using tape memory passed as the
</span><span class=c1>// last argument. This calls a reverse collide GPU kernel.
</span><span class=c1></span><span class=kt>void</span> <span class=nf>rev_kern</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>,</span> <span class=kt>void</span><span class=o>*</span> <span class=n>tape</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>rev_collide</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>,</span> <span class=p>(</span><span class=kt>void</span><span class=o>**</span><span class=p>)</span><span class=n>tape</span><span class=p>);</span>
    <span class=n>cudaFree</span><span class=p>(</span><span class=n>tape</span><span class=p>);</span>
<span class=p>}</span>

<span class=c1>// Here we register the custom forward pass aug_kern and reverse pass rev_kern
</span><span class=c1></span><span class=kt>void</span><span class=o>*</span> <span class=n>__enzyme_register_gradient_kern</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span> <span class=p>(</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>kern</span><span class=p>,</span> <span class=p>(</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>aug_kern</span><span class=p>,</span> <span class=p>(</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>rev_kern</span> <span class=p>};</span>
</code></pre></div><p>Finally, Enzyme has a performance optimization available when creating forward and reverse passes using <code>__enzyme_augmentfwd</code> and <code>__enzyme_reverse</code>. By default, these methods store all variables inside the differentiated function within a generic pointer type (e.g. <code>void*</code>), thereby allowing Enzyme to store as much memory as it needs without issue. This, of course, requires an extra indirection to get to the underlying memory being stored.</p><p>If one knew statically how much memory is required per thread (in this case a single float to store <code>src[idx]</code>), one could tell Enzyme to allocate directly into the tape rather than using this extra level of indirect. This is performed as follows:</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=c1>// Magic Global used to specify how to call Enzyme. In this case, we specify how much memory
</span><span class=c1>// is allocated per invocation within the tape to allow the cache to be inlined.
</span><span class=c1></span><span class=k>extern</span> <span class=n>__device__</span> <span class=kt>int</span> <span class=n>enzyme_allocated</span><span class=p>;</span>

<span class=c1>// A wrapper GPU kernel for calling the forward pass of collide. The wrapper code stores
</span><span class=c1>// the tape generated by Enzyme into a unique location per thread
</span><span class=c1></span><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>aug_collide</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>tape</span><span class=p>)</span>
<span class=p>{</span>
    <span class=n>size_t</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
    <span class=n>tape</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>__enzyme_augmentfwd</span><span class=p>((</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>collide_body</span><span class=p>,</span> <span class=n>enzyme_allocated</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>),</span> <span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>);</span>
<span class=p>}</span>

<span class=c1>// A wrapper GPU kernel for calling the reverse pass of collide. The wrapper code retrieves
</span><span class=c1>// the corresponding tape per thread being executed.
</span><span class=c1></span><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>rev_collide</span><span class=p>(</span> <span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>tape</span><span class=p>)</span>
<span class=p>{</span>
    <span class=n>size_t</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
    <span class=n>__enzyme_reverse</span><span class=p>((</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>collide_body</span><span class=p>,</span> <span class=n>enzyme_allocated</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>),</span> <span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>,</span> <span class=n>tape</span><span class=p>[</span><span class=n>idx</span><span class=p>]);</span>
<span class=p>}</span>

<span class=c1>// The augmented forward pass of the CPU kern call, allocating and returning
</span><span class=c1>// tape memory  needed to compute the reverse pass. This calls a augmented collide
</span><span class=c1>// GPU kernel, passing in a unique 8-byte location to store the tape.
</span><span class=c1></span><span class=kt>void</span><span class=o>*</span> <span class=nf>aug_kern</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>)</span> <span class=p>{</span>
    <span class=kt>float</span><span class=o>*</span> <span class=n>tape</span><span class=p>;</span>
    <span class=n>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>tape</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>)</span> <span class=o>*</span> <span class=cm>/*total number of threads*/</span><span class=mi>100</span><span class=p>);</span>
    <span class=n>aug_collide</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>,</span> <span class=n>tape</span><span class=p>);</span>
    <span class=k>return</span> <span class=p>(</span><span class=kt>void</span><span class=o>*</span><span class=p>)</span><span class=n>tape</span><span class=p>;</span>
<span class=p>}</span>

<span class=c1>// The reverse pass of the CPU kern call, using tape memory passed as the
</span><span class=c1>// last argument. This calls a reverse collide GPU kernel.
</span><span class=c1></span><span class=kt>void</span> <span class=nf>rev_kern</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dsrc</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>ddst</span><span class=p>,</span> <span class=kt>void</span><span class=o>*</span> <span class=n>tape</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>rev_collide</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>dsrc</span><span class=p>,</span> <span class=n>dst</span><span class=p>,</span> <span class=n>ddst</span><span class=p>,</span> <span class=p>(</span><span class=kt>float</span><span class=o>*</span><span class=p>)</span><span class=n>tape</span><span class=p>);</span>
    <span class=n>cudaFree</span><span class=p>(</span><span class=n>tape</span><span class=p>);</span>
<span class=p>}</span>
</code></pre></div><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/getting_started/CallingConvention/ title="Calling Convention"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - Calling Convention</a>
<a class="nav nav-next" href=/getting_started/Faq/ title=FAQ>Next - FAQ <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://enzyme.mit.edu/>Home</a></li><li><a href=/Installation/>Installation</a></li><li><a href=/charter/>Charter</a></li><li class="parent has-sub-menu"><a href=/getting_started/>Getting Started<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/getting_started/UsingEnzyme/>Using Enzyme</a></li><li><a href=/getting_started/CallingConvention/>Calling Convention</a></li><li class=active><a href=/getting_started/CUDAGuide/>CUDA Guide</a></li><li><a href=/getting_started/Faq/>FAQ</a></li></ul></li><li><a href=/talks/>Talks and Related Publications</a></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>