<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Calling Convention - Enzyme AD</title><meta name=description content="Enzyme Automatic Differentiation Framework"><meta name=generator content="Hugo 0.74.3"><link href=//enzyme.mit.edu/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=//enzyme.mit.edu/getting_started/CallingConvention/><link rel=stylesheet href=//enzyme.mit.edu/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=//enzyme.mit.edu/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=//enzyme.mit.edu/js/bundle.js></script><link rel=icon href=/favicon.ico type=image/x-icon><style>:root{}</style></head><body><div class=container><header><h1><div><img src=//enzyme.mit.edu//logo.svg width=40px align=absmiddle>
Enzyme AD</div></h1><p class=description>Enzyme Automatic Differentiation Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://groups.google.com/d/forum/enzyme-dev>Discussion List</a></li></ul></li><li><a href=https://github.com/wsmoses/Enzyme>Source</a></li><li><a href=https://github.com/wsmoses/Enzyme/issues>Bugs</a></li><li><a href=/getting_started/Faq/>FAQ</a></li></ul></nav></div><div class=content-container><main><h1>Calling Convention</h1><h1 id=calling-convention>Calling Convention</h1><p>Enzyme is invoked by calling a function <code>__enzyme_autodiff</code> with the function being differentiated, followed by the corresponding primal and shadow arguments. This will result in the original function being run with the corresponding derivative values being computed.</p><h2 id=function-hooks>Function Hooks&nbsp;<a class=headline-hash href=#function-hooks>¶</a></h2><p>Enzyme replaces all calls to functions that contain the string <code>__enzyme_autodiff</code> with a call to the corresponding derivative. This is done to allow Enzyme to register multiple function signatures.</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=cp>#include</span> <span class=cpf>&lt;stdio.h&gt;</span><span class=cp>
</span><span class=cp></span><span class=k>template</span><span class=o>&lt;</span><span class=k>typename</span> <span class=n>T</span><span class=o>&gt;</span>
<span class=n>T</span> <span class=n>square</span><span class=p>(</span><span class=n>T</span> <span class=n>x</span><span class=p>)</span> <span class=p>{</span> <span class=k>return</span> <span class=n>x</span> <span class=o>*</span> <span class=n>x</span><span class=p>;</span> <span class=p>}</span>

<span class=kt>float</span> <span class=nf>__enzyme_autodiffFloat</span><span class=p>(</span><span class=kt>float</span> <span class=p>(</span><span class=o>*</span><span class=p>)(</span><span class=kt>float</span><span class=p>),</span> <span class=kt>float</span><span class=p>);</span>
<span class=kt>double</span> <span class=nf>__enzyme_autodiffDouble</span><span class=p>(</span><span class=kt>double</span> <span class=p>(</span><span class=o>*</span><span class=p>)(</span><span class=kt>double</span><span class=p>),</span> <span class=kt>double</span><span class=p>);</span>

<span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
  <span class=n>printf</span><span class=p>(</span><span class=s>&#34;float  d/dx %f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>__enzyme_autodiffFloat</span><span class=p>(</span><span class=n>square</span><span class=o>&lt;</span><span class=kt>float</span><span class=o>&gt;</span><span class=p>,</span> <span class=mf>1.0f</span><span class=p>));</span>
  <span class=n>printf</span><span class=p>(</span><span class=s>&#34;double d/dx %f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>__enzyme_autodiffDouble</span><span class=p>(</span><span class=n>square</span><span class=o>&lt;</span><span class=kt>double</span><span class=o>&gt;</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>));</span>
<span class=p>}</span>
</code></pre></div><p>This allows end-library makers to nicely incorporate Enzyme into their workflow through the use of variadic arguments or templates.</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=kt>void</span> <span class=nf>__enzyme_autodiff</span><span class=p>(...);</span>

<span class=k>template</span><span class=o>&lt;</span><span class=k>typename</span> <span class=n>RT</span><span class=p>,</span> <span class=k>typename</span><span class=p>...</span> <span class=n>Args</span><span class=o>&gt;</span>
<span class=n>RT</span> <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=kt>void</span><span class=o>*</span><span class=p>,</span> <span class=n>Args</span><span class=p>...);</span>
</code></pre></div><p>The first argument should either be a function pointer to the code being differentiated, or a cast of the function pointer.</p><h2 id=types>Types&nbsp;<a class=headline-hash href=#types>¶</a></h2><p>Arguments to functions being differentiated are classified to three types:</p><ul><li><em><em>Inactive arguments</em></em> whose values don&rsquo;t impact the derivative computation. An example of this would be an integer representing the size of an array.</li><li><em><em>Output arguments</em></em> are active values whose gradient result is passed as a return value. Examples include floats or doubles.</li><li><em><em>Duplicated arguments</em></em> are active values whose gradient result is stored in a second shadow argument. All active pointer values are duplicated arguments.</li></ul><p>An example program using all three types of these arguments is shown below:</p><div class=highlight><pre class=chroma><code class=language-c data-lang=c><span class=kt>double</span> <span class=nf>sumAndMul</span><span class=p>(</span><span class=kt>double</span><span class=o>*</span> <span class=n>array</span><span class=p>,</span> <span class=n>size_t</span> <span class=n>size</span><span class=p>,</span> <span class=kt>double</span> <span class=n>mul</span><span class=p>)</span> <span class=p>{</span>
  <span class=kt>double</span> <span class=n>sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
  <span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>size</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>sum</span> <span class=o>+=</span> <span class=n>array</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
  <span class=p>}</span>
  <span class=k>return</span> <span class=n>sum</span> <span class=o>*</span> <span class=n>mul</span>
<span class=p>}</span>

<span class=p>...</span>
<span class=kt>double</span> <span class=n>d_mul</span> <span class=o>=</span> <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>sumAndMul</span><span class=p>,</span>
                     <span class=cm>/*duplicated argument*/</span><span class=n>array</span><span class=p>,</span> <span class=n>d_array</span><span class=p>,</span>
                     <span class=cm>/*inactive argument*/</span><span class=n>size</span><span class=p>,</span>
                     <span class=cm>/*output argument*/</span><span class=n>mul</span><span class=p>);</span>
<span class=p>...</span>
</code></pre></div><p>Enzyme will automatically attempt to deduce the classification of argument types. Generally, these rules assume that integer types are inactive arguments, floating-point types are output arguments, and pointer-types are duplicated arguments. A user, however, can explicitly specify the desired classification by using LLVM metadata.</p><p>Inactive arguments are given <code>diffe_const</code> metadata; output arguments are given <code>diffe_out</code> metadata; and duplicated arguments are given <code>diffe_dup</code>.</p><div class=highlight><pre class=chroma><code class=language-llvm data-lang=llvm><span class=nv>%d_mul</span> <span class=p>=</span> <span class=k>tail</span> <span class=k>call</span> <span class=kt>double</span> <span class=vg>@__enzyme_autodiff</span><span class=p>(</span><span class=kt>double</span> <span class=p>(</span><span class=kt>double</span><span class=p>*,</span> <span class=k>i64</span><span class=p>,</span> <span class=kt>double</span><span class=p>)*</span> <span class=vg>@sumAndMul</span><span class=p>,</span> <span class=kt>metadata</span> <span class=nv>!&#34;diffe_dup&#34;</span><span class=p>,</span> <span class=kt>double</span><span class=p>*</span> <span class=nv>%array</span><span class=p>,</span> <span class=kt>double</span><span class=p>*</span> <span class=nv>%d_array</span><span class=p>,</span> <span class=kt>metadata</span> <span class=nv>!&#34;diffe_const&#34;</span><span class=p>,</span> <span class=k>i64</span> <span class=nv>%size</span><span class=p>,</span> <span class=kt>metadata</span> <span class=nv>!&#34;diffe_out&#34;</span><span class=p>,</span> <span class=kt>double</span> <span class=nv>%mul</span><span class=p>)</span>
</code></pre></div><p>To ease the process of writing frontends, Enzyme also will consider loads to global values with specific names as a mechanism to specify argument classification.</p><div class=highlight><pre class=chroma><code class=language-c data-lang=c>
<span class=kt>int</span> <span class=n>diffe_dup</span><span class=p>;</span>
<span class=kt>int</span> <span class=n>diffe_out</span><span class=p>;</span>
<span class=kt>int</span> <span class=n>diffe_const</span><span class=p>;</span>

<span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
  <span class=kt>double</span> <span class=n>d_mul</span> <span class=o>=</span> <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>sumAndMul</span><span class=p>,</span>
                       <span class=n>diffe_dup</span>  <span class=p>,</span> <span class=n>array</span><span class=p>,</span> <span class=n>d_array</span><span class=p>,</span>
                       <span class=n>diffe_const</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span>
                       <span class=n>diffe_out</span>  <span class=p>,</span> <span class=n>mul</span><span class=p>);</span>
<span class=p>}</span>
</code></pre></div><h3 id=shadow-argument-initialization>Shadow argument initialization&nbsp;<a class=headline-hash href=#shadow-argument-initialization>¶</a></h3><p>Enzyme assumes that shadow arguments passed in are already initialized and have the same structure as the primal values. Running Enzyme&rsquo;s generated gradient will increment the shadow value by the amount of the resultant gradient. As a result, this usually means that you want to zero-initialize the shadow prior to calling the gradient.</p><div class=highlight><pre class=chroma><code class=language-c data-lang=c>
<span class=kt>double</span>   <span class=n>array</span><span class=p>[</span><span class=mi>10</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span> <span class=p>...</span> <span class=p>};</span>
<span class=kt>double</span> <span class=n>d_array</span><span class=p>[</span><span class=mi>10</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span> <span class=mf>0.0</span> <span class=p>};</span>

<span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>sumSquare</span><span class=p>,</span>
                  <span class=n>diffe_dup</span><span class=p>,</span> <span class=n>array</span><span class=p>,</span> <span class=n>d_array</span><span class=p>);</span>

<span class=n>printf</span><span class=p>(</span><span class=s>&#34;d(output)/darray[0] = %f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>d_array</span><span class=p>[</span><span class=mi>0</span><span class=p>]);</span>
</code></pre></div><p>For complex datastructures passed as arguments, this requires doing a corresponding initialization of the shadow.</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=k>struct</span> <span class=nc>List</span> <span class=p>{</span>
  <span class=kt>double</span> <span class=n>value</span><span class=p>;</span>
  <span class=n>List</span><span class=o>*</span> <span class=n>next</span><span class=p>;</span>
<span class=p>}</span>

<span class=kt>double</span> <span class=nf>sumList</span><span class=p>(</span><span class=n>List</span><span class=o>*</span> <span class=n>next</span><span class=p>);</span>
<span class=n>List</span><span class=o>*</span> <span class=nf>mklist</span><span class=p>(</span><span class=kt>double</span> <span class=n>value</span><span class=p>,</span> <span class=n>List</span><span class=o>*</span> <span class=n>next</span><span class=p>);</span>

<span class=n>List</span><span class=o>*</span>   <span class=n>list</span> <span class=o>=</span> <span class=k>nullptr</span><span class=p>;</span>
<span class=n>List</span><span class=o>*</span> <span class=n>d_list</span> <span class=o>=</span> <span class=k>nullptr</span><span class=p>;</span>

<span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=mi>5</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
    <span class=n>list</span> <span class=o>=</span> <span class=n>mklist</span><span class=p>(</span>  <span class=n>i</span><span class=p>,</span>   <span class=n>list</span><span class=p>);</span>
  <span class=n>d_list</span> <span class=o>=</span> <span class=n>mklist</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>d_list</span><span class=p>);</span>
<span class=p>}</span>

<span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>sumList</span><span class=p>,</span> <span class=n>list</span><span class=p>,</span> <span class=n>d_list</span><span class=p>);</span>
</code></pre></div><h3 id=result-only-duplicated-argument>Result-only Duplicated Argument&nbsp;<a class=headline-hash href=#result-only-duplicated-argument>¶</a></h3><p>Enzyme also supports a special version of duplicated argument where users only need the computed gradient of the argument and not the value computed in the forward pass. For example, consider the function below that computes a loss function. All the user needs is the gradient of the inputs with respect to the loss and not the loss itself.</p><p>We can instead use the value <code>diffe_dupnoneed</code> to specify this property to Enzyme. This allows Enzyme to do additional optimization.</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=kt>void</span> <span class=nf>neuralNet</span><span class=p>(</span><span class=kt>double</span><span class=o>*</span> <span class=n>loss</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span> <span class=n>W</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span> <span class=n>b</span><span class=p>,</span> <span class=kt>double</span><span class=o>*</span> <span class=n>input</span><span class=p>);</span>

<span class=kt>void</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
  <span class=p>...</span>
  <span class=kt>double</span> <span class=n>loss</span><span class=p>;</span>
  <span class=kt>double</span> <span class=n>d_loss</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>;</span>
  <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>neuralNet</span><span class=p>,</span>
                    <span class=n>diffe_dupnoneed</span><span class=p>,</span> <span class=n>loss</span><span class=p>,</span> <span class=n>d_loss</span><span class=p>,</span>
                    <span class=n>diffe_dup</span><span class=p>,</span>       <span class=n>W</span><span class=p>,</span> <span class=n>d_W</span><span class=p>,</span>
                    <span class=n>diffe_dup</span><span class=p>,</span>       <span class=n>b</span><span class=p>,</span> <span class=n>d_b</span><span class=p>,</span>
                    <span class=n>diffe_const</span><span class=p>,</span>     <span class=n>input</span><span class=p>);</span>
  <span class=c1>// This value is undefined behavior if using diffe_dupnoneed, otherwise
</span><span class=c1></span>  <span class=c1>// it is the same as it would be from calling neuralNet normally.
</span><span class=c1></span>  <span class=n>printf</span><span class=p>(</span><span class=s>&#34;loss=%f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>loss</span><span class=p>);</span>

  <span class=n>printf</span><span class=p>(</span><span class=s>&#34;d_b[0]=%f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>d_b</span><span class=p>[</span><span class=mi>0</span><span class=p>]);</span>
<span class=p>}</span>
</code></pre></div><h3 id=wrapper-functions>Wrapper Functions&nbsp;<a class=headline-hash href=#wrapper-functions>¶</a></h3><p>When passing complicated types as arguments, it&rsquo;s sometimes desirable to explicitly pass them as duplicated argument. This can be accomplished by creating a wrapper function that takes a pointer argument and simply calls a function with the reference value.</p><div class=highlight><pre class=chroma><code class=language-cpp data-lang=cpp><span class=k>class</span> <span class=nc>MyClass</span><span class=p>;</span>

<span class=n>MyClass</span> <span class=nf>compute</span><span class=p>(</span><span class=n>MyClass</span><span class=o>&amp;</span><span class=p>);</span>

<span class=kt>void</span> <span class=nf>wrapper</span><span class=p>(</span><span class=n>MyClass</span><span class=o>*</span> <span class=n>in</span><span class=p>,</span> <span class=n>MyClass</span><span class=o>*</span> <span class=n>out</span><span class=p>)</span> <span class=p>{</span>
  <span class=o>*</span><span class=n>out</span> <span class=o>=</span> <span class=n>compute</span><span class=p>(</span><span class=o>*</span><span class=n>in</span><span class=p>);</span>
<span class=p>}</span>

<span class=n>MyClass</span> <span class=nf>d_compute</span><span class=p>(</span><span class=n>MyClass</span><span class=o>&amp;</span> <span class=n>in</span><span class=p>)</span> <span class=p>{</span>
  <span class=n>MyClass</span> <span class=n>d_in</span><span class=p>(</span><span class=mf>0.0</span><span class=p>);</span>
  <span class=n>MyClass</span> <span class=n>out</span><span class=p>;</span>
  <span class=n>MyClass</span> <span class=n>d_out</span><span class=p>(</span><span class=mf>1.0</span><span class=p>);</span>
  <span class=n>__enzyme_autodiff</span><span class=p>(</span><span class=n>wrapper</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>in</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>out</span><span class=p>,</span> <span class=n>d_out</span><span class=p>);</span>
  <span class=k>return</span> <span class=n>d_in</span><span class=p>;</span>
<span class=p>}</span>
</code></pre></div><h2 id=globals>Globals&nbsp;<a class=headline-hash href=#globals>¶</a></h2><p>All global variables that are active must have their shadow explicitly specified in LLVM. This is done by attaching metadata that specifies what the shadow of that global is</p><div class=highlight><pre class=chroma><code class=language-llvm data-lang=llvm><span class=vg>@global</span> <span class=p>=</span> <span class=k>external</span> <span class=k>local_unnamed_addr</span> <span class=k>global</span> <span class=kt>double</span><span class=p>,</span> <span class=k>align</span> <span class=m>8</span><span class=p>,</span> <span class=nv>!enzyme_shadow</span> <span class=p>!{</span><span class=kt>double</span><span class=p>*</span> <span class=vg>@dglobal</span><span class=p>}</span>
<span class=vg>@dglobal</span> <span class=p>=</span> <span class=k>external</span> <span class=k>local_unnamed_addr</span> <span class=k>global</span> <span class=kt>double</span><span class=p>,</span> <span class=k>align</span>
</code></pre></div><h2 id=custom-gradients>Custom gradients&nbsp;<a class=headline-hash href=#custom-gradients>¶</a></h2><p>Functions can be given a custom gradient by attaching two pieces of metadata. These pieces of metadata specify an augmented forward pass that saves any state necessary for the reverse pass and the reverse pass that computes the gradient.</p><p>Presently, custom gradients are only supported where Enzyme&rsquo;s default argument classification is correct. This means that the all floating-point arguments must be treated as active output arguments, all pointer arguments must be treated as active duplicated arguments, and all integers are inactive arguments.</p><p>Both functions has the same arguments the forward pass along with any duplicated arguments mixed in. The gradient function then has a differential return value if the original function&rsquo;s return value is an output argument. The final argument is a custom &ldquo;tape&rdquo; type that can be used to pass information from the forward to the reverse pass.</p><p>The return type of the augmented forward pass is a struct type containing first the tape type, followed by the original return type, if any. If the return type is a duplicated type, then there is a third argument which contains the shadow of the return.</p><p>The return type of the reverse pass is a struct containing derivatives of all of the output arguments.</p><div class=highlight><pre class=chroma><code class=language-llvm data-lang=llvm><span class=k>define</span> <span class=k>internal</span> <span class=p>{</span> <span class=p>{},</span> <span class=kt>double</span> <span class=p>}</span> <span class=vg>@augment_add2</span><span class=p>(</span><span class=kt>double</span> <span class=nv>%x</span><span class=p>)</span> <span class=p>{</span>
<span class=nl>entry:</span>
  <span class=nv>%add</span> <span class=p>=</span> <span class=k>fadd</span> <span class=k>fast</span> <span class=kt>double</span> <span class=nv>%x</span><span class=p>,</span> <span class=m>2.000000e+00</span>
  <span class=nv>%struct1</span> <span class=p>=</span> <span class=k>insertvalue</span> <span class=p>{</span> <span class=p>{},</span> <span class=kt>double</span> <span class=p>}</span> <span class=k>undef</span><span class=p>,</span> <span class=kt>double</span> <span class=nv>%add</span><span class=p>,</span> <span class=m>1</span>
  <span class=k>ret</span> <span class=p>{</span> <span class=p>{},</span> <span class=kt>double</span> <span class=p>}</span> <span class=nv>%struct1</span>
<span class=p>}</span>

<span class=k>define</span> <span class=k>internal</span> <span class=p>{</span> <span class=kt>double</span> <span class=p>}</span> <span class=vg>@gradient_add2</span><span class=p>(</span><span class=kt>double</span> <span class=nv>%x</span><span class=p>,</span> <span class=kt>double</span> <span class=nv>%differet</span><span class=p>,</span> <span class=p>{}</span> <span class=nv>%tapeArg</span><span class=p>)</span> <span class=p>{</span>
<span class=nl>entry:</span>
  <span class=nv>%struct1</span> <span class=p>=</span> <span class=k>insertvalue</span> <span class=p>{</span> <span class=kt>double</span> <span class=p>}</span> <span class=k>undef</span><span class=p>,</span> <span class=kt>double</span> <span class=nv>%differet</span><span class=p>,</span> <span class=m>0</span>
  <span class=k>ret</span> <span class=p>{</span> <span class=kt>double</span> <span class=p>}</span> <span class=nv>%struct1</span>
<span class=p>}</span>

<span class=k>declare</span> <span class=nv>!enzyme_augment</span> <span class=p>!{{</span> <span class=p>{},</span> <span class=kt>double</span> <span class=p>}</span> <span class=p>(</span><span class=kt>double</span><span class=p>)*</span> <span class=vg>@augment_add2</span><span class=p>}</span> <span class=nv>!enzyme_gradient</span> <span class=p>!{{</span> <span class=kt>double</span> <span class=p>}</span> <span class=p>(</span><span class=kt>double</span><span class=p>,</span> <span class=kt>double</span><span class=p>,</span> <span class=p>{})*</span> <span class=vg>@gradient_add2</span><span class=p>}</span> <span class=kt>double</span> <span class=vg>@add2</span><span class=p>(</span><span class=kt>double</span> <span class=nv>%x</span><span class=p>)</span>
</code></pre></div><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/getting_started/UsingEnzyme/ title="Using Enzyme"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - Using Enzyme</a>
<a class="nav nav-next" href=/getting_started/Faq/ title=FAQ>Next - FAQ <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=//enzyme.mit.edu/>Home</a></li><li><a href=/Installation/>Installation</a></li><li class="parent has-sub-menu"><a href=/getting_started/>Getting Started<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/getting_started/UsingEnzyme/>Using Enzyme</a></li><li class=active><a href=/getting_started/CallingConvention/>Calling Convention</a></li><li><a href=/getting_started/Faq/>FAQ</a></li></ul></li><li><a href=/talks/>Talks and Related Publications</a></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>