using LLVM
using LLVM.Interop
import MCAnalyzer: irgen

const _mod = Ref{LLVM.Module}()

@enum Diffe begin
   Duplicate = 1
   Output = 2
   Constant = 3
end

function hasfieldcount(@nospecialize(dt))
    try
        fieldcount(dt)
    catch
        return false
    end
    return true
end

function whatType(@nospecialize(dt))
    if <:(dt, Array)
        sub = whatType(eltype(dt))
        if sub == "diffe_dup"
            return "diffe_dup"
        elseif sub == "diffe_out"
            return "diffe_dup"
        else
            @assert(sub == "diffe_const")
            return "diffe_const"
        end
    end
    if <:(dt, Real)
        return "diffe_out"
    end
    if <:(dt, Int)
        return "diffe_const"
    end
    if <:(dt, String)
        return "diffe_const"
    end

    if ! hasfieldcount(dt)
        # just be safe for now
        return "diffe_dup"
    end
    @assert(hasfieldcount(dt))
    @assert(isstructtype(dt))
    passpointer = true
    if passpointer
        ty = "diffe_const"
        for (ft, fn) in zip(fieldtypes(dt), fieldnames(dt))
            sub = whatType(ft)
            if sub == "diffe_dup"
                ty = "diffe_dup"
            elseif sub == "diffe_out"
                ty = "diffe_dup"
            else
                @assert(sub == "diffe_const")
            end
        end
        return ty
    else
        ty = "diffe_const"
        for (ft, fn) in zip(fieldtypes(dt), fieldnames(dt))
            sub = whatType(ft)
            if sub == "diffe_dup"
                ty = "diffe_dup"
            elseif sub == "diffe_out"
                if ty != "diffe_dup"
                    ty = "diffe_out"
                end
            else
                @assert(sub == "diffe_const")
            end
        end
        return ty
    end
end
    
@generated function autodiff(f, args...)
    # Obtain the function and all it's dependencies in one handy module
    diffetypes = []
    autodifftypes = Type[f]
    i = 1
    while i <= length(args)
        push!(autodifftypes, args[i])
        dt = whatType(args[i])
        push!(diffetypes, dt)
        if dt == "diffe_dup"
            i+=1
        end
        i+=1
    end
    mod, ccf = irgen(Tuple{autodifftypes...}) 

    ctx = context(mod)
    rettype = convert(LLVMType, Float64)
    
    #argtypes2 = LLVMType[convert(LLVMType, T, true) for T in args]
    argtypes2 = LLVMType[]
    
    i = 1
    j = 1
    Base.println(typeof(ccf))
    Base.println(typeof(llvmtype(ccf)))
    Base.println(llvmtype(ccf))
    orig_params = parameters(ccf)
    for p in orig_params
        Base.println(llvmtype(p))
        push!(argtypes2, llvmtype(p))
        if diffetypes[i] == "diffe_dup"
            push!(argtypes2, llvmtype(p))
            i+=2
        else
            i+=1
        end
    end
    Base.println(argtypes2)
    

    # TODO get function type from ccf
    ft2  = LLVM.FunctionType(rettype, argtypes2)

    # create a wrapper Function that we will inline into the llvmcall
    # generated by in the end `call_function`
    llvmf = LLVM.Function(mod, "", ft2)
    push!(function_attributes(llvmf), EnumAttribute("alwaysinline", 0, ctx))
    linkage!(llvmf, LLVM.API.LLVMPrivateLinkage)

    # Create the FunctionType and funtion decleration for the intrinsic
    pt       = LLVM.PointerType(LLVM.Int8Type(ctx))
    ftd      = LLVM.FunctionType(rettype, LLVMType[pt], true)
    autodiff = LLVM.Function(mod, "llvm.autodiff.p0i8", ftd)

    params = LLVM.Value[]
    i = 1
    j = 1
    llvm_params = parameters(llvmf)
    while j <= length(args)
        push!(params, MDString(diffetypes[i]))
        if diffetypes[i] == "diffe_dup"
            push!(params, llvm_params[j])
            j+=1
        end
        push!(params, llvm_params[j])
        j+=1
        i+=1
    end

    Builder(ctx) do builder
        entry = BasicBlock(llvmf, "entry", ctx)
        position!(builder, entry)

        tc = bitcast!(builder, ccf, pt)
        pushfirst!(params, tc)

        val = call!(builder, autodiff, params)

        #if T === Nothing
        #    ret!(builder)
        #else
            ret!(builder, val)
        #end
    end

    _mod[] = mod
    @show mod
    _args = (:(args[$i]) for i in 1:length(args))
    call_function(llvmf, Float64, Tuple{args...}, Expr(:tuple, _args...))
end

using InteractiveUtils

if false
@noinline function square(X::Array{Float64,1})
    X[1] * X[1]
end

function cube(X::Array{Float64,1})
    square(X) * X[1]
end

ar = [1.0]
arp = [0.0]
@show cube(ar)
@code_llvm optimize=false cube(ar)
@show autodiff(cube, ar, arp)
@show arp

end


if true
function sum(X::Array{Float64,1})
  acc = zero(eltype(X))
  #@simd for x in X
  for x in X
      acc += x
  end
return acc
end
function sqcube(x::Array{Float64,1})
  sum(x)
  #sum(x .* x)
end

ar = [1.0, 2.0]
arp = [0.0, 0.0]
@show sqcube(ar)
#@code_llvm optimize=false sqcube(ar)
@show autodiff(sqcube, ar, arp)
@show arp

end

if false

using LinearAlgebra

mutable struct LinearRegression
    # These values will be implicitly learned
    weights::Matrix{Float64}
    bias::Float64

    # These values will not be learned
    name::String
end

LinearRegression(nparams, name) = LinearRegression(randn(1, nparams), 0.0, name)
import Base: zero
zero(model::LinearRegression) = LinearRegression(zero(model.weights), zero(model.bias), model.name)

# Our linear prediction looks very familiar; w*X + b
function predict(model::LinearRegression, X)
    return model.weights * X .+ model.bias
end

# Our "loss" that must be minimized is the l2 norm  between our current
# prediction and our ground-truth Y
function loss(model::LinearRegression, X, Y)
    return norm(predict(model, X) .- Y, 2)
end

const weights_gt = Float64[1.0, 2.7, 0.3, 1.2]'
const bias_gt = 0.4

# Generate a dataset of many observations
const Xn = randn(length(weights_gt), 10000)
const Y = weights_gt * Xn .+ bias_gt

# Add a little bit of noise to `X` so that we do not have an exact solution,
# but must instead do a least-squares fit:
const X = Xn .+ 0.001.*randn(size(Xn))


model = LinearRegression(size(X, 1), "Example")

function testmodel_old(m::LinearRegression)
    return loss(m, X[:,1], Y[1])
end

function testmodel(m::LinearRegression)
    #vals = (m.weights * X[:,1] .+ model.bias) .- Y[1]
    #return (vals .* vals)[[1]]
    return Float64(norm((m.weights * X[:,1] .+ m.bias) .- Y[1], 2))
end
outp = zero(model)

using InteractiveUtils
@code_llvm testmodel(model)

autodiff(testmodel, model, outp)

println(outp)
end
